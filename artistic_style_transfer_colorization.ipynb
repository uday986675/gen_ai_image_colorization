{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tkinter as tk\n",
        "from tkinter import filedialog, ttk, messagebox\n",
        "import numpy as np\n",
        "from PIL import Image, ImageTk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import cv2\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "class ArtisticColorizationSystem:\n",
        "    def __init__(self):\n",
        "        # Available artistic styles\n",
        "        self.styles = {\n",
        "            \"Van Gogh (Starry Night)\": \"vangogh\",\n",
        "            \"Monet (Water Lilies)\": \"monet\",\n",
        "            \"Picasso (Cubist)\": \"picasso\",\n",
        "            \"Hokusai (The Great Wave)\": \"hokusai\",\n",
        "            \"Rembrandt (Chiaroscuro)\": \"rembrandt\",\n",
        "            \"Modern Pop Art\": \"popart\",\n",
        "            \"Impressionist\": \"impressionist\",\n",
        "            \"No Style (Natural Color)\": \"natural\"\n",
        "        }\n",
        "\n",
        "        # Initialize models\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.colorization_model = None\n",
        "        self.style_transfer_model = None\n",
        "        self.vgg19 = None\n",
        "\n",
        "        # GUI components\n",
        "        self.root = None\n",
        "        self.input_image = None\n",
        "        self.output_image = None\n",
        "        self.style_var = None\n",
        "\n",
        "        self.setup_models()\n",
        "\n",
        "    def setup_models(self):\n",
        "        \"\"\"Initialize the colorization and style transfer models\"\"\"\n",
        "        print(\"Setting up models...\")\n",
        "\n",
        "        # Setup VGG19 for style extraction (simplified)\n",
        "        self.vgg19 = models.vgg19(pretrained=True).features.to(self.device).eval()\n",
        "\n",
        "        # For the demo, we'll use a simplified approach\n",
        "        # In production, you would load pretrained colorization and style transfer models\n",
        "        print(\"Models initialized (simulated for demo)\")"
      ],
      "metadata": {
        "id": "-wyOtnYE8SFk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CORE MODELS:"
      ],
      "metadata": {
        "id": "8n9WiPkR60e9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ColorizationNet(nn.Module):\n",
        "    \"\"\"U-Net based architecture for colorization\"\"\"\n",
        "    def __init__(self, input_channels=1, output_channels=2):\n",
        "        super(ColorizationNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder1 = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.encoder2 = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder2 = nn.Sequential(\n",
        "            nn.Conv2d(384, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "\n",
        "        self.decoder1 = nn.Sequential(\n",
        "            nn.Conv2d(192, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, output_channels, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder path\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(enc1)\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(enc2)\n",
        "\n",
        "        # Decoder path with skip connections\n",
        "        dec2 = self.decoder2(torch.cat([bottleneck, enc2], dim=1))\n",
        "        dec1 = self.decoder1(torch.cat([dec2, enc1], dim=1))\n",
        "\n",
        "        return dec1"
      ],
      "metadata": {
        "id": "okkcSyU4yXzC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleTransfer:\n",
        "    \"\"\"Apply artistic styles to colorized images\"\"\"\n",
        "    @staticmethod\n",
        "    def extract_features(image, model):\n",
        "        \"\"\"Extract features from VGG19 layers\"\"\"\n",
        "        features = []\n",
        "        x = image\n",
        "        for i, layer in enumerate(model):\n",
        "            x = layer(x)\n",
        "            if i in {1, 6, 11, 20, 29}:  # Selected layers for style/content\n",
        "                features.append(x)\n",
        "        return features\n",
        "\n",
        "    @staticmethod\n",
        "    def gram_matrix(tensor):\n",
        "        \"\"\"Compute Gram matrix for style representation\"\"\"\n",
        "        _, d, h, w = tensor.size()\n",
        "        tensor = tensor.view(d, h * w)\n",
        "        gram = torch.mm(tensor, tensor.t())\n",
        "        return gram\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_style(colorized_img, style_name):\n",
        "        \"\"\"Apply specific artistic style to image\"\"\"\n",
        "        # Convert to LAB and then to RGB for processing\n",
        "        if isinstance(colorized_img, np.ndarray):\n",
        "            colorized_img = torch.from_numpy(colorized_img).float().to('cpu')\n",
        "\n",
        "        # In production, this would use actual style transfer models\n",
        "        # For demo, we apply simulated style effects\n",
        "        img_np = colorized_img.numpy().transpose(1, 2, 0)\n",
        "\n",
        "        # Apply different filters based on selected style\n",
        "        if style_name == \"vangogh\":\n",
        "            # Simulate Van Gogh's brush strokes\n",
        "            return StyleTransfer.apply_vangogh_effect(img_np)\n",
        "        elif style_name == \"monet\":\n",
        "            # Simulate Monet's impressionism\n",
        "            return StyleTransfer.apply_monet_effect(img_np)\n",
        "        elif style_name == \"popart\":\n",
        "            # Simulate pop art effect\n",
        "            return StyleTransfer.apply_popart_effect(img_np)\n",
        "        elif style_name == \"natural\":\n",
        "            # No style, return original\n",
        "            return img_np\n",
        "        else:\n",
        "            # Default mild artistic effect\n",
        "            return StyleTransfer.apply_default_artistic(img_np)\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_vangogh_effect(img):\n",
        "        \"\"\"Simulate Van Gogh's style\"\"\"\n",
        "        # This would be replaced with actual neural style transfer\n",
        "        # For demo, using OpenCV filters\n",
        "        result = cv2.stylization(img, sigma_s=60, sigma_r=0.6)\n",
        "        result = cv2.detailEnhance(result, sigma_s=10, sigma_r=0.15)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_monet_effect(img):\n",
        "        \"\"\"Simulate Monet's impressionist style\"\"\"\n",
        "        result = cv2.edgePreservingFilter(img, flags=1, sigma_s=60, sigma_r=0.4)\n",
        "        # Add warm impressionist tone\n",
        "        result = cv2.applyColorMap(result, cv2.COLORMAP_SUMMER)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_popart_effect(img):\n",
        "        \"\"\"Apply pop art style\"\"\"\n",
        "        # Posterization effect\n",
        "        result = cv2.medianBlur(img, 3)\n",
        "        Z = result.reshape((-1, 3))\n",
        "        Z = np.float32(Z)\n",
        "        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "        K = 8\n",
        "        _, label, center = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
        "        center = np.uint8(center)\n",
        "        result = center[label.flatten()].reshape((result.shape))\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_default_artistic(img):\n",
        "        \"\"\"Default artistic enhancement\"\"\"\n",
        "        result = cv2.detailEnhance(img, sigma_s=10, sigma_r=0.15)\n",
        "        return result"
      ],
      "metadata": {
        "id": "niM4_UH2yv51"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main application entry point\"\"\"\n",
        "    print(\"Starting Artistic Style Transfer in Colorization System...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Check for required packages\n",
        "    try:\n",
        "        import tkinter\n",
        "        import torch\n",
        "        import torchvision\n",
        "        import PIL\n",
        "        import cv2\n",
        "        import numpy as np\n",
        "    except ImportError as e:\n",
        "        print(f\"Error: Missing required package - {e}\")\n",
        "        print(\"Please install required packages:\")\n",
        "        print(\"pip install torch torchvision pillow opencv-python numpy\")\n",
        "        return\n",
        "\n",
        "    # Create and run the system\n",
        "    system = ArtisticColorizationSystem()\n",
        "    gui = ArtisticColorizationGUI(system)\n",
        "    gui.run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "5vRGLE3O_qPF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "outputId": "2862df66-5221-47d0-9d50-dcb0ec7ab112"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Artistic Style Transfer in Colorization System...\n",
            "============================================================\n",
            "Setting up models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 407M/548M [00:02<00:00, 200MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3795331897.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3795331897.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Create and run the system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0msystem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArtisticColorizationSystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mgui\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArtisticColorizationGUI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mgui\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3737987395.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3737987395.py\u001b[0m in \u001b[0;36msetup_models\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Setup VGG19 for style extraction (simplified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg19\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg19\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# For the demo, we'll use a simplified approach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword_only_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36minner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweights_param\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_weights_arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mvgg19\u001b[0;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG19_Weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_vgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"E\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36m_vgg\u001b[0;34m(cfg, batch_norm, weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_hash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/_api.py\u001b[0m in \u001b[0;36mget_state_dict\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_state_dict_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHASH_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# r is Optional[Match[str]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0mhash_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0mdownload_url_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_legacy_zip_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    756\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[possibly-undefined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhash_prefix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m                     \u001b[0msha256\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[possibly-undefined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GUI Interface\n"
      ],
      "metadata": {
        "id": "5Mqn2vFv6ptC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ArtisticColorizationGUI:\n",
        "    \"\"\"Graphical User Interface for the system\"\"\"\n",
        "    def __init__(self, system):\n",
        "        self.system = system\n",
        "        self.root = tk.Tk()\n",
        "        self.root.title(\"Artistic Style Transfer in Colorization\")\n",
        "        self.root.geometry(\"1200x800\")\n",
        "\n",
        "        # Configure style\n",
        "        self.setup_styles()\n",
        "\n",
        "        # Variables\n",
        "        self.input_image_path = None\n",
        "        self.output_image = None\n",
        "        self.style_var = tk.StringVar(value=\"No Style (Natural Color)\")\n",
        "\n",
        "        # Create GUI\n",
        "        self.create_widgets()\n",
        "\n",
        "    def setup_styles(self):\n",
        "        \"\"\"Setup ttk styles\"\"\"\n",
        "        style = ttk.Style()\n",
        "        style.theme_use('clam')\n",
        "\n",
        "        # Custom colors\n",
        "        bg_color = \"#2c3e50\"\n",
        "        fg_color = \"#ecf0f1\"\n",
        "        accent_color = \"#3498db\"\n",
        "\n",
        "        style.configure('Title.TLabel',\n",
        "                       font=('Helvetica', 16, 'bold'),\n",
        "                       background=bg_color,\n",
        "                       foreground=fg_color)\n",
        "\n",
        "        style.configure('Subtitle.TLabel',\n",
        "                       font=('Helvetica', 12),\n",
        "                       background=bg_color,\n",
        "                       foreground=accent_color)\n",
        "\n",
        "    def create_widgets(self):\n",
        "        \"\"\"Create all GUI widgets\"\"\"\n",
        "        # Main container\n",
        "        main_frame = ttk.Frame(self.root)\n",
        "        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
        "\n",
        "        # Title\n",
        "        title_label = ttk.Label(main_frame,\n",
        "                               text=\"Artistic Style Transfer in Colorization\",\n",
        "                               style='Title.TLabel')\n",
        "        title_label.pack(pady=(0, 20))\n",
        "\n",
        "        # Content frame\n",
        "        content_frame = ttk.Frame(main_frame)\n",
        "        content_frame.pack(fill=tk.BOTH, expand=True)\n",
        "\n",
        "        # Left panel - Input\n",
        "        left_panel = ttk.LabelFrame(content_frame, text=\"Input Image\", padding=10)\n",
        "        left_panel.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)\n",
        "\n",
        "        # Input image display\n",
        "        self.input_canvas = tk.Canvas(left_panel, width=400, height=400,\n",
        "                                     bg='#34495e', relief=tk.SUNKEN)\n",
        "        self.input_canvas.pack(pady=10)\n",
        "        self.input_canvas.create_text(200, 200,\n",
        "                                     text=\"No Image Selected\",\n",
        "                                     fill=\"white\",\n",
        "                                     font=('Helvetica', 12))\n",
        "\n",
        "        # Upload button\n",
        "        upload_btn = ttk.Button(left_panel, text=\"Upload Grayscale Image\",\n",
        "                               command=self.upload_image)\n",
        "        upload_btn.pack(pady=10)\n",
        "\n",
        "        # Style selection\n",
        "        style_frame = ttk.LabelFrame(content_frame, text=\"Artistic Style Selection\", padding=10)\n",
        "        style_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)\n",
        "\n",
        "        # Style dropdown\n",
        "        style_label = ttk.Label(style_frame, text=\"Choose Artistic Style:\")\n",
        "        style_label.pack(anchor=tk.W, pady=(0, 5))\n",
        "\n",
        "        self.style_combo = ttk.Combobox(style_frame,\n",
        "                                       textvariable=self.style_var,\n",
        "                                       values=list(self.system.styles.keys()),\n",
        "                                       state=\"readonly\")\n",
        "        self.style_combo.pack(fill=tk.X, pady=(0, 20))\n",
        "\n",
        "        # Style preview images (simulated)\n",
        "        preview_label = ttk.Label(style_frame, text=\"Style Previews:\")\n",
        "        preview_label.pack(anchor=tk.W, pady=(10, 5))\n",
        "\n",
        "        preview_frame = ttk.Frame(style_frame)\n",
        "        preview_frame.pack(fill=tk.BOTH, expand=True)\n",
        "\n",
        "        # Right panel - Output\n",
        "        right_panel = ttk.LabelFrame(content_frame, text=\"Colorized & Styled Output\", padding=10)\n",
        "        right_panel.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5)\n",
        "\n",
        "        # Output image display\n",
        "        self.output_canvas = tk.Canvas(right_panel, width=400, height=400,\n",
        "                                      bg='#34495e', relief=tk.SUNKEN)\n",
        "        self.output_canvas.pack(pady=10)\n",
        "        self.output_canvas.create_text(200, 200,\n",
        "                                      text=\"Processed Image will appear here\",\n",
        "                                      fill=\"white\",\n",
        "                                      font=('Helvetica', 12))\n",
        "\n",
        "        # Process button\n",
        "        process_btn = ttk.Button(right_panel, text=\"Colorize & Apply Style\",\n",
        "                                command=self.process_image,\n",
        "                                style='Accent.TButton')\n",
        "        process_btn.pack(pady=20)\n",
        "\n",
        "        # Progress bar\n",
        "        self.progress = ttk.Progressbar(right_panel, mode='indeterminate')\n",
        "        self.progress.pack(fill=tk.X, pady=10)\n",
        "\n",
        "        # Status bar\n",
        "        self.status_var = tk.StringVar(value=\"Ready to process images\")\n",
        "        status_bar = ttk.Label(main_frame, textvariable=self.status_var,\n",
        "                              relief=tk.SUNKEN, anchor=tk.W)\n",
        "        status_bar.pack(side=tk.BOTTOM, fill=tk.X)\n",
        "\n",
        "        # Control buttons frame\n",
        "        control_frame = ttk.Frame(main_frame)\n",
        "        control_frame.pack(fill=tk.X, pady=10)\n",
        "\n",
        "        ttk.Button(control_frame, text=\"Save Output\",\n",
        "                  command=self.save_output).pack(side=tk.LEFT, padx=5)\n",
        "        ttk.Button(control_frame, text=\"Reset\",\n",
        "                  command=self.reset).pack(side=tk.LEFT, padx=5)\n",
        "        ttk.Button(control_frame, text=\"Exit\",\n",
        "                  command=self.root.quit).pack(side=tk.RIGHT, padx=5)\n",
        "\n",
        "    def upload_image(self):\n",
        "        \"\"\"Handle image upload\"\"\"\n",
        "        file_path = filedialog.askopenfilename(\n",
        "            title=\"Select Grayscale Image\",\n",
        "            filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png *.bmp *.tiff\")]\n",
        "        )\n",
        "\n",
        "        if file_path:\n",
        "            try:\n",
        "                self.input_image_path = file_path\n",
        "                img = Image.open(file_path)\n",
        "\n",
        "                # Convert to grayscale if needed\n",
        "                if img.mode != 'L':\n",
        "                    img = img.convert('L')\n",
        "\n",
        "                # Resize for display\n",
        "                img.thumbnail((380, 380))\n",
        "                self.input_display_img = ImageTk.PhotoImage(img)\n",
        "\n",
        "                # Update canvas\n",
        "                self.input_canvas.delete(\"all\")\n",
        "                self.input_canvas.create_image(200, 200,\n",
        "                                             image=self.input_display_img)\n",
        "\n",
        "                self.status_var.set(f\"Loaded: {os.path.basename(file_path)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                messagebox.showerror(\"Error\", f\"Failed to load image: {str(e)}\")\n",
        "\n",
        "    def process_image(self):\n",
        "        \"\"\"Process image with colorization and style transfer\"\"\"\n",
        "        if not self.input_image_path:\n",
        "            messagebox.showwarning(\"No Image\", \"Please upload an image first!\")\n",
        "            return\n",
        "\n",
        "        # Start progress bar\n",
        "        self.progress.start()\n",
        "        self.status_var.set(\"Processing image...\")\n",
        "        self.root.update()\n",
        "\n",
        "        try:\n",
        "            # Load and preprocess image\n",
        "            img = Image.open(self.input_image_path).convert('L')\n",
        "            img_array = np.array(img)\n",
        "\n",
        "            # Simulate colorization (in production, use actual model)\n",
        "            colorized = self.simulate_colorization(img_array)\n",
        "\n",
        "            # Get selected style\n",
        "            style_name = self.system.styles[self.style_var.get()]\n",
        "\n",
        "            # Apply style transfer\n",
        "            styled = StyleTransfer.apply_style(colorized, style_name)\n",
        "\n",
        "            # Convert to PIL Image for display\n",
        "            if styled.shape[0] == 3:  # CHW format\n",
        "                styled = styled.transpose(1, 2, 0)\n",
        "\n",
        "            output_img = Image.fromarray((styled * 255).astype(np.uint8))\n",
        "            output_img.thumbnail((380, 380))\n",
        "            self.output_display_img = ImageTk.PhotoImage(output_img)\n",
        "\n",
        "            # Update canvas\n",
        "            self.output_canvas.delete(\"all\")\n",
        "            self.output_canvas.create_image(200, 200,\n",
        "                                          image=self.output_display_img)\n",
        "\n",
        "            self.status_var.set(f\"Applied {self.style_var.get()} style successfully!\")\n",
        "            self.output_image = styled\n",
        "\n",
        "        except Exception as e:\n",
        "            messagebox.showerror(\"Processing Error\", f\"Failed to process image: {str(e)}\")\n",
        "            self.status_var.set(\"Processing failed\")\n",
        "\n",
        "        finally:\n",
        "            self.progress.stop()\n",
        "\n",
        "    def simulate_colorization(self, grayscale_img):\n",
        "        \"\"\"Simulate colorization process (demo purposes)\"\"\"\n",
        "        # In production, this would use the actual ColorizationNet\n",
        "        # For demo, create a color version from grayscale\n",
        "        if len(grayscale_img.shape) == 2:\n",
        "            # Convert to 3-channel with color tint\n",
        "            colorized = np.stack([grayscale_img] * 3, axis=-1)\n",
        "\n",
        "            # Apply color tint based on image content\n",
        "            mean_val = np.mean(grayscale_img)\n",
        "            if mean_val < 85:\n",
        "                # Dark image - warm tones\n",
        "                colorized[:, :, 0] *= 0.9  # Reduce blue\n",
        "                colorized[:, :, 1] *= 1.1  # Enhance green\n",
        "            elif mean_val > 170:\n",
        "                # Bright image - cool tones\n",
        "                colorized[:, :, 0] *= 1.2  # Enhance blue\n",
        "            else:\n",
        "                # Mid tones - balanced\n",
        "                colorized[:, :, 0] *= 1.1  # Slight blue tint\n",
        "                colorized[:, :, 2] *= 0.9  # Reduce red\n",
        "\n",
        "            # Normalize\n",
        "            colorized = np.clip(colorized, 0, 255).astype(np.uint8)\n",
        "            colorized = colorized.transpose(2, 0, 1) / 255.0\n",
        "\n",
        "            return colorized\n",
        "        return grayscale_img\n",
        "\n",
        "    def save_output(self):\n",
        "        \"\"\"Save processed image\"\"\"\n",
        "        if self.output_image is None:\n",
        "            messagebox.showwarning(\"No Output\", \"No processed image to save!\")\n",
        "            return\n",
        "\n",
        "        file_path = filedialog.asksaveasfilename(\n",
        "            defaultextension=\".png\",\n",
        "            filetypes=[(\"PNG files\", \"*.png\"),\n",
        "                      (\"JPEG files\", \"*.jpg\"),\n",
        "                      (\"All files\", \"*.*\")]\n",
        "        )\n",
        "\n",
        "        if file_path:\n",
        "            try:\n",
        "                # Convert to proper format and save\n",
        "                if isinstance(self.output_image, np.ndarray):\n",
        "                    if self.output_image.shape[0] == 3:\n",
        "                        img_to_save = (self.output_image.transpose(1, 2, 0) * 255).astype(np.uint8)\n",
        "                    else:\n",
        "                        img_to_save = (self.output_image * 255).astype(np.uint8)\n",
        "\n",
        "                    Image.fromarray(img_to_save).save(file_path)\n",
        "                    self.status_var.set(f\"Image saved to {os.path.basename(file_path)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                messagebox.showerror(\"Save Error\", f\"Failed to save image: {str(e)}\")\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the application\"\"\"\n",
        "        self.input_image_path = None\n",
        "        self.output_image = None\n",
        "        self.input_canvas.delete(\"all\")\n",
        "        self.output_canvas.delete(\"all\")\n",
        "        self.input_canvas.create_text(200, 200,\n",
        "                                     text=\"No Image Selected\",\n",
        "                                     fill=\"white\")\n",
        "        self.output_canvas.create_text(200, 200,\n",
        "                                      text=\"Processed Image will appear here\",\n",
        "                                      fill=\"white\")\n",
        "        self.style_var.set(\"No Style (Natural Color)\")\n",
        "        self.status_var.set(\"Ready to process images\")\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Start the GUI application\"\"\"\n",
        "        self.root.mainloop()"
      ],
      "metadata": {
        "id": "G730SYN86W6A"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53d1395c"
      },
      "source": [
        "The `TclError` means that a graphical user interface (GUI) application is trying to run in a headless environment (like Colab) without a display server. To fix this, we will modify the `main` function to not initialize the `tkinter` GUI directly. If you need a graphical output, consider alternative visualization methods like displaying images directly in the notebook output using `matplotlib` or saving them to files."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio torch torchvision Pillow opencv-python numpy\n"
      ],
      "metadata": {
        "id": "efwXIxlwi1qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gradio as gr\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def apply_style_effect(image, style_name):\n",
        "    \"\"\"Apply artistic style effects using OpenCV\"\"\"\n",
        "    img_array = np.array(image)\n",
        "\n",
        "    if style_name == \"Van Gogh\":\n",
        "        # Stylization effect\n",
        "        result = cv2.stylization(img_array, sigma_s=60, sigma_r=0.6)\n",
        "    elif style_name == \"Monet\":\n",
        "        # Impressionist effect\n",
        "        result = cv2.edgePreservingFilter(img_array, flags=1, sigma_s=60, sigma_r=0.4)\n",
        "    elif style_name == \"Picasso\":\n",
        "        # Cubist-like effect\n",
        "        result = cv2.detailEnhance(img_array, sigma_s=10, sigma_r=0.15)\n",
        "    elif style_name == \"Hokusai\":\n",
        "        # Woodblock print effect\n",
        "        result = cv2.applyColorMap(img_array, cv2.COLORMAP_SUMMER)\n",
        "    elif style_name == \"Pop Art\":\n",
        "        # Posterization effect\n",
        "        Z = img_array.reshape((-1,3))\n",
        "        Z = np.float32(Z)\n",
        "        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "        K = 8\n",
        "        _, label, center = cv2.kmeans(Z, K, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
        "        center = np.uint8(center)\n",
        "        result = center[label.flatten()].reshape((img_array.shape))\n",
        "    else:  # Natural\n",
        "        result = img_array\n",
        "\n",
        "    return Image.fromarray(result)\n",
        "\n",
        "def colorize_and_style(input_image, style_choice):\n",
        "    \"\"\"Main processing function\"\"\"\n",
        "    # Convert to grayscale first (simulating colorization input)\n",
        "    if input_image.mode != 'L':\n",
        "        grayscale = input_image.convert('L')\n",
        "        # Convert back to RGB for demo (simulating colorization)\n",
        "        colorized = Image.merge('RGB', [grayscale, grayscale, grayscale])\n",
        "    else:\n",
        "        colorized = Image.merge('RGB', [input_image, input_image, input_image])\n",
        "\n",
        "    # Apply selected style\n",
        "    styled_image = apply_style_effect(colorized, style_choice)\n",
        "\n",
        "    return styled_image\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=colorize_and_style,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Upload Image\"),\n",
        "        gr.Dropdown(\n",
        "            choices=[\"Van Gogh\", \"Monet\", \"Picasso\", \"Hokusai\", \"Pop Art\", \"Natural\"],\n",
        "            value=\"Natural\",\n",
        "            label=\"Select Artistic Style\"\n",
        "        )\n",
        "    ],\n",
        "    outputs=gr.Image(type=\"pil\", label=\"Styled Output\"),\n",
        "    title=\"üé® Artistic Style Transfer in Colorization\",\n",
        "    description=\"Upload an image and select an artistic style to apply. The system will colorize (simulated) and apply the selected style.\",\n",
        "    examples=[\n",
        "        [\"https://images.unsplash.com/photo-1506744038136-46273834b3fb\", \"Van Gogh\"],\n",
        "        [\"https://images.unsplash.com/photo-1519681393784-d120267933ba\", \"Monet\"]\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Launch in Colab\n",
        "iface.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "91Gt4eUC-OZc",
        "outputId": "397ead54-f3d7-41e8-b4fd-ee85ed7110f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://db5eb5de61e2f6be76.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://db5eb5de61e2f6be76.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://db5eb5de61e2f6be76.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}